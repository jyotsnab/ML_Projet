{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from workalendar.usa import Illinois\n",
    "from sklearn.cluster import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done data selection...\n",
      "begin clustering...\n",
      "done clustering...\n",
      "done all of feature engineering...\n"
     ]
    }
   ],
   "source": [
    "calendar = Illinois()   \n",
    "\n",
    "n_clusters = 50\n",
    "\n",
    "def feature_selection(initial_csv):\n",
    "    columns_to_use = ['Date', 'X Coordinate', 'Y Coordinate']\n",
    "    data = pd.read_csv(initial_csv, sep=',', encoding='utf8', usecols=columns_to_use)\n",
    "    return data\n",
    "\n",
    "def date_decompose(df):\n",
    "    df = df.copy()\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['Hour'] = df['Date'].dt.hour\n",
    "    df['Weekday'] = df['Date'].dt.weekday\n",
    "    ## Adding day off informations does not seem to be useful eventually\n",
    "    #df['Weekend'] = (df['Weekday'] < 5).astype(int) ## not pretty effective\n",
    "    #df['Holiday'] = df['Date'].apply(lambda x: int(calendar.is_holiday(x))) ## same\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "    df = df.dropna()\n",
    "    df = date_decompose(df)\n",
    "    df['Crimes'] = 1\n",
    "    coordX = df['X Coordinate']\n",
    "    coordY = df['Y Coordinate']\n",
    "    coords = pd.concat([coordX, coordY],axis=1).as_matrix()\n",
    "    ## The mini batch KMeans is appropriate here as it stays time efficient even for huge dataset\n",
    "    modCluster = MiniBatchKMeans(n_clusters=n_clusters, init='k-means++', max_iter=100, batch_size=100, verbose=0, compute_labels=True,\n",
    "                                 random_state=None, tol=0.0, max_no_improvement=10, init_size=None, n_init=3, reassignment_ratio=0.01)\n",
    "    print('begin clustering...')\n",
    "    labels = modCluster.fit_predict(coords)\n",
    "    print('done clustering...')\n",
    "        \n",
    "    df['Cluster'] = labels\n",
    "    df = df[['Year', 'Month', 'Day', 'Weekday', 'Hour', 'Cluster', 'Crimes']]\n",
    "    df = df.groupby(['Year', 'Month', 'Day', 'Weekday', 'Hour', 'Cluster']).sum().reset_index()\n",
    "    \n",
    "    years = df['Year']\n",
    "    months = df['Month']\n",
    "    days = df['Day']\n",
    "    weekday = df['Weekday']\n",
    "    hours = df['Hour']\n",
    "    cluster = df['Cluster']\n",
    "    crimes = df['Crimes']\n",
    "    \n",
    "    df = pd.concat([years, months, days, weekday, hours, cluster, crimes], axis=1)\n",
    "        \n",
    "    ## Two years of data, from 2008 to 2009 included\n",
    "    #df = df.query('Year >= 2008 and Year <= 2009')\n",
    "    ## One year of data, 2008\n",
    "    df = df.query('Year == 2008')\n",
    "        \n",
    "    return df\n",
    "\n",
    "#inputfile = 'data/stack.csv' ## Sample dataset\n",
    "inputfile = 'data/Crimes_-_2001_to_present.csv' \n",
    "\n",
    "dataSelected = feature_selection(inputfile)\n",
    "print(\"done data selection...\")\n",
    "dataTransformed = feature_engineering(dataSelected)\n",
    "print(\"done all of feature engineering...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00%\n",
      "22.77%\n",
      "45.54%\n",
      "68.31%\n",
      "91.07%\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "\"\"\" \n",
    "    The goal here is to generate new rows matching (date, location) couples for which no crime occured. \n",
    "    In order to do so, we have to consider all (date, location) couples in a given range of dates \n",
    "    (all dates between the first and last chosen dates) \n",
    "\"\"\"\n",
    "\n",
    "years = dataTransformed['Year'].values\n",
    "years = np.reshape(years, (len(years), 1))\n",
    "months = pd.get_dummies(dataTransformed['Month']).values\n",
    "days = dataTransformed['Day'].values\n",
    "days = np.reshape(days, (len(days), 1))\n",
    "weekdays = pd.get_dummies(dataTransformed['Weekday']).values\n",
    "hours = dataTransformed['Hour'].values\n",
    "hours = np.reshape(hours, (len(hours), 1))\n",
    "clusters = pd.get_dummies(dataTransformed['Cluster']).values\n",
    "crimes = dataTransformed['Crimes'].values\n",
    "crimes = np.reshape(crimes, (len(crimes), 1))\n",
    "\n",
    "hours_clusters_crimes = np.concatenate((years, months, days, weekdays, hours, clusters, crimes), axis=1)\n",
    "\n",
    "date_begin = dataTransformed.head(1).ix[:, :3].values[0]\n",
    "date_end = dataTransformed.tail(1).ix[:, :3].values[0]\n",
    "\n",
    "date_begin_dt = datetime(date_begin[0], date_begin[1], date_begin[2])\n",
    "date_end_dt = datetime(date_end[0], date_end[1], date_end[2])\n",
    "\n",
    "diff = date_end_dt - date_begin_dt\n",
    "theorical_size = (diff.days + 1) * n_clusters * 24\n",
    "\n",
    "date_cursor = date_begin_dt\n",
    "mat = []\n",
    "compt_hours = 0\n",
    "for i in range(theorical_size):\n",
    "    if i%n_clusters == 0 and i != 0:\n",
    "        date_cursor += timedelta(hours=1)\n",
    "    \n",
    "    tok_clusters = i % n_clusters\n",
    "      \n",
    "    year = date_cursor.year  \n",
    "    month = [0.]*(date_cursor.month -1) + [1.] + [0.]*(12 - date_cursor.month)\n",
    "    day = date_cursor.day\n",
    "    weekday = [0.]*date_cursor.weekday() + [1.] + [0.]*(7-date_cursor.weekday()-1)\n",
    "    hour = date_cursor.hour\n",
    "    vec_cluster = [0.]*tok_clusters + [1.] + [0.]*(n_clusters-tok_clusters-1)\n",
    "    \n",
    "    mat.append(np.asarray([year] + month + [day] + weekday + [hour] + vec_cluster + [0.]))\n",
    "    \n",
    "mat = np.asarray(mat)\n",
    "\n",
    "index = []\n",
    "compt = 0\n",
    "\n",
    "for i in range(len(mat)):\n",
    "    if np.array_equal(mat[i, :-1], hours_clusters_crimes[compt, :-1]):\n",
    "        index.append([i, hours_clusters_crimes[compt, -1]])\n",
    "        compt += 1\n",
    "        if compt == len(hours_clusters_crimes):\n",
    "            break\n",
    "    else:\n",
    "        pass\n",
    "    if i % 100000 == 0:\n",
    "        state = i / len(mat) * 100\n",
    "        print(\"{0:.2f}\".format(state) + '%')\n",
    "\n",
    "for i in range(len(index)):\n",
    "    mat[index[i][0], -1] = index[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputfile = 'data/complete_grouped_crimes.csv'\n",
    "pd.DataFrame(mat).to_csv(outputfile, sep=';', mode='w', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import platform\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def model(X, y):\n",
    "    print('Training the model..')\n",
    "            \n",
    "    reg = Pipeline([\n",
    "            ('rfr', RFR(n_estimators=30, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "             max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, \n",
    "             verbose=0, warm_start=False))\n",
    "            ]) \n",
    "    reg.fit(X, y)\n",
    "    print('Done training the model..')       \n",
    "    return reg\n",
    "    \n",
    "def predict(X, reg):    \n",
    "    y_pred = reg.predict(X)         \n",
    "    return y_pred\n",
    "\n",
    "def parse_file(file):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(file, 'r') as data:\n",
    "        data.readline()\n",
    "        for line in data:\n",
    "            row = line.split(';')\n",
    "            for i in range(len(row)):\n",
    "                row[i] = float(row[i])\n",
    "            X.append(row[:len(row)-1])\n",
    "            y.append(row[len(row)-1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done parsing..\n",
      "Done splitting labeled/unlabeled ones..\n",
      "Training the model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dalkio/anaconda3/lib/python3.5/site-packages/sklearn/pipeline.py:270: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training the model..\n",
      "Done training..\n",
      "--------------------------\n",
      "Mean Squared Error = 1.18908343453\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "        \n",
    "def crossValidationError(file='data/complete_grouped_crimes.csv'):\n",
    "    p_unlabelled = 0.2\n",
    "    X, y = parse_file(file)   \n",
    "    print(\"Done parsing..\")\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(y)\n",
    "    Y = Y.reshape(len(Y),1)\n",
    "    \n",
    "    X_lab, X_unlab, y_lab, y_unlab = train_test_split(X, Y, test_size=p_unlabelled, random_state=57)\n",
    "    print(\"Done splitting labeled/unlabeled ones..\")\n",
    "    \n",
    "    mod = model(X_lab, y_lab)\n",
    "    print('Done training..')\n",
    "    \n",
    "    yPred = predict(X_unlab, mod)\n",
    "    \n",
    "    mseError = mean_squared_error(y_unlab, yPred)\n",
    "    return mseError\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crossvalidationError = crossValidationError()\n",
    "    print('--------------------------') \n",
    "    print('Mean Squared Error = ' + str(crossvalidationError))\n",
    "    print('--------------------------') "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
